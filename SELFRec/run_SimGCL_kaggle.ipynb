{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d690e21",
   "metadata": {},
   "source": [
    "# Run MSBEGCL on Kaggle\n",
    "\n",
    "This notebook sets up the environment, compiles the necessary C++ mining tools, prepares the data, runs the mining algorithm to generate bicliques, and finally trains the MSBEGCL recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, time, shutil, struct\n",
    "\n",
    "# --- Configuration ---\n",
    "repo_url = 'https://github.com/yangzeha/MSBEGCL.git'\n",
    "repo_dir = 'MSBEGCL'\n",
    "model_name = 'MSBEGCL'\n",
    "dataset_name = 'yelp2018'\n",
    "\n",
    "# 1. Clean and Clone Repository (Silent)\n",
    "if os.path.exists(repo_dir):\n",
    "    try:\n",
    "        shutil.rmtree(repo_dir)\n",
    "    except Exception as e:\n",
    "        subprocess.run(['rm', '-rf', repo_dir], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "try:\n",
    "    subprocess.run(['git', 'clone', '-b', 'master', repo_url], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    sys.exit(1)\n",
    "\n",
    "# 2. Setup Directories (Silent)\n",
    "if os.path.basename(os.getcwd()) != repo_dir:\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "# [Robustness Fix]: Auto-detect nested structure\n",
    "roots = os.listdir('.')\n",
    "target_structure_found = False\n",
    "possible_subdirs = ['.', 'MSBEGCL', 'msbegcl', repo_dir]\n",
    "\n",
    "for d in possible_subdirs:\n",
    "    if d == '.': path_to_check = '.'\n",
    "    else:\n",
    "        path_to_check = d\n",
    "        if not os.path.exists(d) or not os.path.isdir(d): continue\n",
    "    contents = os.listdir(path_to_check)\n",
    "    if 'SELFRec' in contents and 'Similar-Biclique-Idx' in contents:\n",
    "        if d != '.': os.chdir(d)\n",
    "        target_structure_found = True\n",
    "        break\n",
    "\n",
    "if not target_structure_found:\n",
    "    found = False\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        if 'SELFRec' in dirs:\n",
    "            os.chdir(root)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(\"CRITICAL ERROR: Could not locate SELFRec directory anywhere.\")\n",
    "\n",
    "selfrec_path = 'SELFRec'\n",
    "msbe_path = 'Similar-Biclique-Idx'\n",
    "\n",
    "# 3. Install Dependencies (Silent)\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'PyYAML==6.0.2', 'scipy==1.14.1', '-q'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'faiss-cpu', '-q'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 4. Compile C++ Mining Tools (Silent)\n",
    "sparsez_dir = 'sparsehash'\n",
    "if not os.path.exists(sparsez_dir):\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/sparsehash/sparsehash.git'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    cwd_backup = os.getcwd()\n",
    "    os.chdir(sparsez_dir)\n",
    "    try:\n",
    "        subprocess.run(['chmod', '+x', 'configure'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) \n",
    "        subprocess.run(['./configure'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        subprocess.run(['make'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    finally:\n",
    "        os.chdir(cwd_backup)\n",
    "\n",
    "# Compile msbe\n",
    "msbe_src = os.path.join(msbe_path, 'main.cpp')\n",
    "msbe_exe = './msbe'\n",
    "if not os.path.exists(msbe_src):\n",
    "    print(f\"CRITICAL ERROR: Source file {msbe_src} not found!\")\n",
    "else:\n",
    "    subprocess.run(['g++', '-w', '-O3', msbe_src, '-o', msbe_exe, '-I', msbe_path, '-I', 'sparsehash/src', '-D_PrintResults_', '-D_CheckResults_'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    subprocess.run(['chmod', '+x', msbe_exe], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# 5. Data Preprocessing\n",
    "print(f'\\n--- Preprocessing {dataset_name} for Mining ---')\n",
    "train_file = os.path.join(selfrec_path, 'dataset', dataset_name, 'train.txt')\n",
    "mining_graph_txt = 'graph.txt'\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print(f\"CRITICAL ERROR: Data file {train_file} not found!\")\n",
    "else:\n",
    "    users = set()\n",
    "    items = set()\n",
    "    edges = []\n",
    "    with open(train_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                u, i = parts[0], parts[1]\n",
    "                users.add(u)\n",
    "                items.add(i)\n",
    "                edges.append((u, i))\n",
    "\n",
    "    try:\n",
    "        sorted_users = sorted(list(users), key=lambda x: int(x))\n",
    "        sorted_items = sorted(list(items), key=lambda x: int(x))\n",
    "    except:\n",
    "        sorted_users = sorted(list(users))\n",
    "        sorted_items = sorted(list(items))\n",
    "\n",
    "    u_map = {u: idx for idx, u in enumerate(sorted_users)}\n",
    "    i_map = {i: idx for idx, i in enumerate(sorted_items)}\n",
    "\n",
    "    n1 = len(users)\n",
    "    n2 = len(items)\n",
    "   \n",
    "    print(f'Preprocessing graph with {n1} users, {n2} items, {len(edges)} edges.')\n",
    "    \n",
    "    total_nodes = n1 + n2\n",
    "    adj = [[] for _ in range(total_nodes)]\n",
    "    edge_count = 0\n",
    "    \n",
    "    for u, i in edges:\n",
    "        uid = u_map[u]\n",
    "        iid = i_map[i] + n1\n",
    "        adj[uid].append(iid)\n",
    "        adj[iid].append(uid)\n",
    "        edge_count += 2\n",
    "        \n",
    "    for k in range(total_nodes):\n",
    "        adj[k].sort()\n",
    "        \n",
    "    degree_file = 'graph_b_degree.bin'\n",
    "    with open(degree_file, 'wb') as f:\n",
    "        f.write(struct.pack('I', 4))\n",
    "        f.write(struct.pack('I', n1))\n",
    "        f.write(struct.pack('I', n2))\n",
    "        f.write(struct.pack('I', edge_count))\n",
    "        degrees = [len(adj[k]) for k in range(total_nodes)]\n",
    "        f.write(struct.pack(f'{total_nodes}I', *degrees))\n",
    "        \n",
    "    adj_file = 'graph_b_adj.bin'\n",
    "    with open(adj_file, 'wb') as f:\n",
    "        flat_adj = []\n",
    "        for k in range(total_nodes):\n",
    "            flat_adj.extend(adj[k])\n",
    "        f.write(struct.pack(f'{edge_count}I', *flat_adj))\n",
    "        \n",
    "    print(f\"Generated binary graph files.\")\n",
    "    \n",
    "    with open(mining_graph_txt, 'w') as f:\n",
    "        f.write(\"dummy\")\n",
    "\n",
    "# 6. Run Mining\n",
    "print('\\n--- Mining Bicliques (Strict Mode) ---')\n",
    "sim_threshold = 0.3    # Updated per request\n",
    "size_threshold = 3     # Updated per request\n",
    "\n",
    "if os.path.exists(msbe_exe) and os.path.exists(mining_graph_txt):\n",
    "    print('Building Index...')\n",
    "    subprocess.run([msbe_exe, mining_graph_txt, '1', '1', str(sim_threshold), 'GRL3'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    print('Enumerating...')\n",
    "    raw_bicliques_file = 'bicliques_raw.txt'\n",
    "    with open(raw_bicliques_file, 'w') as outfile:\n",
    "        subprocess.run([\n",
    "            msbe_exe, mining_graph_txt, \n",
    "            '0', '1', str(sim_threshold), 'GRL3', \n",
    "            '1', 'GRL3', \n",
    "            '0', '0', 'heu', \n",
    "            '4', str(sim_threshold), str(size_threshold), '2'\n",
    "        ], stdout=outfile, stderr=subprocess.DEVNULL, check=True)\n",
    "    \n",
    "    if os.path.exists(raw_bicliques_file):\n",
    "        size = os.path.getsize(raw_bicliques_file)\n",
    "        print(f\"Mining output file size: {size} bytes\")\n",
    "else:\n",
    "    print(\"Skipping mining due to compliation or data failure.\")\n",
    "\n",
    "# 7. Process Bicliques -> Model Format\n",
    "print('\\n--- Formatting Bicliques for Model ---')\n",
    "final_biclique_path = os.path.join(selfrec_path, 'dataset', dataset_name, 'bicliques.txt')\n",
    "count = 0\n",
    "\n",
    "if os.path.exists('bicliques_raw.txt'):\n",
    "    with open('bicliques_raw.txt', 'r') as fr, open(final_biclique_path, 'w') as fw:\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            clean_line = line.replace('|', ' ').replace(',', ' ').replace(':', ' ')\n",
    "            tokens = clean_line.split()\n",
    "            current_users = []\n",
    "            current_items = []\n",
    "            for t in tokens:\n",
    "                if not t.isdigit(): continue\n",
    "                nid = int(t)\n",
    "                if nid < n1:\n",
    "                    if nid < len(sorted_users):\n",
    "                        current_users.append(sorted_users[nid])\n",
    "                else:\n",
    "                    iid = nid - n1\n",
    "                    if iid >= 0 and iid < len(sorted_items):\n",
    "                        current_items.append(sorted_items[iid])\n",
    "            if len(current_users) > 0 and len(current_items) > 0:\n",
    "                fw.write(f\"{' '.join(current_users)} | {' '.join(current_items)}\\n\")\n",
    "                count += 1\n",
    "    print(f\"Processed {count} bicliques into {final_biclique_path}\")\n",
    "else:\n",
    "    print(\"Warning: bicliques_raw.txt not found.\")\n",
    "\n",
    "# 8. Update Configuration\n",
    "conf_path = os.path.join(selfrec_path, 'conf', 'MSBEGCL.yaml')\n",
    "\n",
    "if os.path.exists(conf_path):\n",
    "    with open(conf_path, 'r') as f:\n",
    "        conf_content = f.read()\n",
    "\n",
    "    new_path = f'./dataset/{dataset_name}/bicliques.txt'\n",
    "    import re\n",
    "    \n",
    "    # Core Parameters\n",
    "    conf_content = re.sub(r'biclique\\.file:.*', f'biclique.file: {new_path}', conf_content)\n",
    "    conf_content = re.sub(r'lambda:.*', 'lambda: 0.3', conf_content)\n",
    "    conf_content = re.sub(r'gamma:.*', 'gamma: 0.8', conf_content)\n",
    "    conf_content = re.sub(r'n_layer:.*', 'n_layer: 2', conf_content)\n",
    "    \n",
    "    # Ensure all new params exist or replace them\n",
    "    updates = {\n",
    "        'tau:': 'tau: 0.15',\n",
    "        'eps:': 'eps: 0.1',\n",
    "        'alpha:': 'alpha: 0.1',\n",
    "        'local_k:': 'local_k: 3',\n",
    "        'top_k_neighbors:': 'top_k_neighbors: 5',\n",
    "        'sim_threshold:': 'sim_threshold: 0.3', \n",
    "        'warmup_epochs:': 'warmup_epochs: 10',\n",
    "        'use_dynamic_weight:': 'use_dynamic_weight: true',\n",
    "        'batch.size:': 'batch.size: 4096'\n",
    "    }\n",
    "\n",
    "    for key, val in updates.items():\n",
    "        if key.strip(':') in conf_content:\n",
    "            conf_content = re.sub(rf'{key.strip(\":\")}:.*', val, conf_content)\n",
    "        else:\n",
    "            conf_content += f'\\n{val}'\n",
    "\n",
    "    with open(conf_path, 'w') as f:\n",
    "        f.write(conf_content)\n",
    "    print(\"Updated MSBEGCL.yaml with Advanced Config.\")\n",
    "\n",
    "# [Patching] Write Helper Files\n",
    "print('--- Patching Model Code in Notebook ---')\n",
    "\n",
    "# 1. denoising_helper.py\n",
    "denoising_code = r'''\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def filter_large_bicliques(bicliques, max_size=30):\n",
    "    \"\"\"过滤过大的二团（可能是噪声）\"\"\"\n",
    "    filtered = []\n",
    "    for users, items in bicliques:\n",
    "        if len(users) <= max_size and len(items) <= max_size:\n",
    "            filtered.append((users, items))\n",
    "    return filtered\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def prune_low_similarity_clusters(bicliques, interaction_mat, min_sim=0.2):\n",
    "    \"\"\"基于内部相似度修剪二团\"\"\"\n",
    "    pruned = []\n",
    "    for users, items in bicliques:\n",
    "        user_sims = []\n",
    "        user_list = list(users)\n",
    "        for i in range(len(user_list)):\n",
    "            for j in range(i+1, len(user_list)):\n",
    "                # Handle sparse matrix efficiently\n",
    "                try:\n",
    "                    u1_idx = interaction_mat[user_list[i]].indices\n",
    "                    u2_idx = interaction_mat[user_list[j]].indices\n",
    "                except:\n",
    "                    # Fallback for non-sparse or different structure\n",
    "                    try:\n",
    "                         u1_idx = np.where(interaction_mat[user_list[i]] > 0)[0]\n",
    "                         u2_idx = np.where(interaction_mat[user_list[j]] > 0)[0]\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                u1_interactions = set(u1_idx)\n",
    "                u2_interactions = set(u2_idx)\n",
    "                sim = compute_jaccard_similarity(u1_interactions, u2_interactions)\n",
    "                user_sims.append(sim)\n",
    "        \n",
    "        avg_sim = np.mean(user_sims) if user_sims else 0\n",
    "        if avg_sim >= min_sim:\n",
    "            pruned.append((users, items))\n",
    "    return pruned\n",
    "\n",
    "def build_enhanced_neighbor_dict(bicliques, user_map, item_map, top_k=10):\n",
    "    user_neighbors = defaultdict(list)\n",
    "    item_neighbors = defaultdict(list)\n",
    "    \n",
    "    for users, items in bicliques:\n",
    "        user_ids = [user_map[u] for u in users if u in user_map]\n",
    "        item_ids = [item_map[i] for i in items if i in item_map]\n",
    "        \n",
    "        for u in user_ids:\n",
    "            for other_u in user_ids:\n",
    "                if u != other_u: user_neighbors[u].append(other_u)\n",
    "        \n",
    "        for i in item_ids:\n",
    "            for other_i in item_ids:\n",
    "                if i != other_i: item_neighbors[i].append(other_i)\n",
    "    \n",
    "    enhanced_user = {}\n",
    "    enhanced_item = {}\n",
    "    \n",
    "    for u, neighbors in user_neighbors.items():\n",
    "        unique_neighbors = list(set(neighbors))\n",
    "        if len(unique_neighbors) > top_k:\n",
    "            enhanced_user[u] = random.sample(unique_neighbors, top_k)\n",
    "        else:\n",
    "            enhanced_user[u] = unique_neighbors\n",
    "    \n",
    "    for i, neighbors in item_neighbors.items():\n",
    "        unique_neighbors = list(set(neighbors))\n",
    "        if len(unique_neighbors) > top_k:\n",
    "            enhanced_item[i] = random.sample(unique_neighbors, top_k)\n",
    "        else:\n",
    "            enhanced_item[i] = unique_neighbors\n",
    "    \n",
    "    return enhanced_user, enhanced_item\n",
    "'''\n",
    "denoise_path = os.path.join(selfrec_path, 'util', 'denoising_helper.py')\n",
    "with open(denoise_path, 'w') as f:\n",
    "    f.write(denoising_code)\n",
    "print(\"Created util/denoising_helper.py\")\n",
    "\n",
    "# 2. msbe_helper.py\n",
    "msbe_helper_code = r'''\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from util.denoising_helper import filter_large_bicliques, prune_low_similarity_clusters, build_enhanced_neighbor_dict\n",
    "\n",
    "def load_msbe_neighbors(file_path, user_map, item_map, interaction_mat=None, sim_threshold=0.3):\n",
    "    \"\"\"加载并增强二团邻居\"\"\"\n",
    "    user_neighbors = {}\n",
    "    item_neighbors = {}\n",
    "    \n",
    "    print(f\"Loading bicliques from {file_path}, sim_threshold={sim_threshold}\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        bicliques = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) < 2: continue\n",
    "                \n",
    "                users = [u.strip() for u in parts[0].split() if u.strip() in user_map]\n",
    "                items = [i.strip() for i in parts[1].split() if i.strip() in item_map]\n",
    "                \n",
    "                if len(users) > 0 and len(items) > 0:\n",
    "                    bicliques.append((users, items))\n",
    "        \n",
    "        print(f\"Original bicliques: {len(bicliques)}\")\n",
    "        \n",
    "        # Step 1: Filter\n",
    "        filtered = filter_large_bicliques(bicliques, max_size=30)\n",
    "        print(f\"Filtered bicliques: {len(filtered)}\")\n",
    "        \n",
    "        # Step 2: Prune\n",
    "        if interaction_mat is not None:\n",
    "            # Note: This step can be slow. If interaction_mat is sparse CSR, logic in helper handles it.\n",
    "            try:\n",
    "                pruned = prune_low_similarity_clusters(filtered, interaction_mat, min_sim=sim_threshold)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Pruning failed ({e}), using filtered only.\")\n",
    "                pruned = filtered\n",
    "            print(f\"Pruned bicliques: {len(pruned)}\")\n",
    "        else:\n",
    "            pruned = filtered\n",
    "        \n",
    "        # Step 3: Global Neighbor Dict\n",
    "        user_neighbors, item_neighbors = build_enhanced_neighbor_dict(\n",
    "            pruned, user_map, item_map, top_k=15\n",
    "        )\n",
    "        \n",
    "        u_c = sum(1 for v in user_neighbors.values() if len(v) > 0)\n",
    "        i_c = sum(1 for v in item_neighbors.values() if len(v) > 0)\n",
    "        print(f\"Enhanced Neighbors: {u_c} users, {i_c} items.\")\n",
    "    \n",
    "    return user_neighbors, item_neighbors\n",
    "'''\n",
    "helper_path = os.path.join(selfrec_path, 'util', 'msbe_helper.py')\n",
    "with open(helper_path, 'w') as f:\n",
    "    f.write(msbe_helper_code)\n",
    "print(\"Patched util/msbe_helper.py\")\n",
    "\n",
    "# 3. MSBEGCL.py\n",
    "model_code = r'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, InfoNCE\n",
    "import random\n",
    "import numpy as np\n",
    "from util.msbe_helper import load_msbe_neighbors\n",
    "\n",
    "class MSBEGCL(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(MSBEGCL, self).__init__(conf, training_set, test_set)\n",
    "        args = self.config['MSBEGCL']\n",
    "        \n",
    "        # Core Params\n",
    "        self.n_layers = int(args['n_layer'])\n",
    "        self.eps = float(args.get('eps', 0.1))\n",
    "        self.tau = float(args.get('tau', 0.2))\n",
    "        \n",
    "        # Loss Weights\n",
    "        self.base_cl_rate = float(args['lambda'])  # Uniformity\n",
    "        self.base_msb_rate = float(args['gamma'])  # Structure\n",
    "        self.local_rate = float(args.get('alpha', 0.1)) # Local Sim\n",
    "        self.top_k = int(args.get('top_k_neighbors', 5))\n",
    "        self.local_k = int(args.get('local_k', 3))\n",
    "        \n",
    "        # Dynamic Schedule\n",
    "        self.warmup_epochs = int(args.get('warmup_epochs', 10))\n",
    "        self.use_dynamic_weight = args.get('use_dynamic_weight', True)\n",
    "        \n",
    "        print(f\"MSBEGCL Config: lambda={self.base_cl_rate}, gamma={self.base_msb_rate}, tau={self.tau}\")\n",
    "        print(f\"Local Sim: alpha={self.local_rate}, local_k={self.local_k}\")\n",
    "        \n",
    "        self.model = MSBEGCL_Encoder(self.data, self.emb_size, self.eps, self.n_layers)\n",
    "        \n",
    "        # Load Biclique Neighbors\n",
    "        self.biclique_file = args['biclique.file']\n",
    "        self.sim_threshold = float(args.get('sim_threshold', 0.3))\n",
    "        self.user_msb_neighbors, self.item_msb_neighbors = self.load_denoised_neighbors()\n",
    "        \n",
    "        # Cache for Local Similarity\n",
    "        self.user_sim_cache = {}\n",
    "        self.item_sim_cache = {}\n",
    "        \n",
    "    def load_denoised_neighbors(self):\n",
    "        # We pass interaction_mat (typically SciPy sparse CSR) to the helper\n",
    "        user_neighbors, item_neighbors = load_msbe_neighbors(\n",
    "            self.biclique_file,\n",
    "            self.data.user,\n",
    "            self.data.item,\n",
    "            self.data.interaction_mat, \n",
    "            self.sim_threshold\n",
    "        )\n",
    "        return user_neighbors, item_neighbors\n",
    "\n",
    "    def compute_local_similarity(self, embeddings, indices, cache_dict):\n",
    "        \"\"\"Batch-wise Local Similarity Calculation\"\"\"\n",
    "        batch_embeddings = embeddings[indices]\n",
    "        sim_matrix = torch.matmul(batch_embeddings, batch_embeddings.T)\n",
    "        \n",
    "        # Mask self\n",
    "        mask = torch.eye(len(indices), device=embeddings.device).bool()\n",
    "        sim_matrix = sim_matrix.masked_fill(mask, -1e9)\n",
    "        return sim_matrix\n",
    "    \n",
    "    def local_sim_loss(self, view1, view2, indices, neighbor_dict, cache_key='user'):\n",
    "        if len(indices) < 2: return torch.tensor(0.0, device=view1.device)\n",
    "        \n",
    "        # Compute similarities within views\n",
    "        sim_matrix_v1 = self.compute_local_similarity(view1, indices, self.user_sim_cache)\n",
    "        sim_matrix_v2 = self.compute_local_similarity(view2, indices, self.user_sim_cache)\n",
    "        \n",
    "        # Select top-k similar nodes (mining hard positives/local context)\n",
    "        k_val = min(self.local_k, len(indices)-1)\n",
    "        topk_v1 = torch.topk(sim_matrix_v1, k=k_val, dim=1)\n",
    "        topk_v2 = torch.topk(sim_matrix_v2, k=k_val, dim=1)\n",
    "        \n",
    "        loss = 0.0\n",
    "        batch_size = len(indices)\n",
    "        \n",
    "        # 1. Structural Anchor Contrast (if available)\n",
    "        for i in range(batch_size):\n",
    "            node_idx = indices[i].item()\n",
    "            if cache_key == 'user' and node_idx in self.user_msb_neighbors:\n",
    "                msb_neighbors = self.user_msb_neighbors[node_idx]\n",
    "                if len(msb_neighbors) > 0:\n",
    "                    sampled = random.sample(msb_neighbors, min(self.top_k, len(msb_neighbors)))\n",
    "                    # Structure view from view2\n",
    "                    struct_emb = torch.mean(view2[sampled], dim=0)\n",
    "                    anchor = view1[node_idx]\n",
    "                    \n",
    "                    # Contrastive Term\n",
    "                    pos_score = torch.exp(torch.dot(anchor, struct_emb) / self.tau)\n",
    "                    self_score = torch.exp(torch.dot(anchor, view2[node_idx]) / self.tau)\n",
    "                    loss += -torch.log(pos_score / (self_score + 1e-8))\n",
    "\n",
    "        # 2. Local Neighborhood Alignment\n",
    "        for k in range(k_val):\n",
    "            pos_v1 = view1[topk_v1.indices[:, k]]\n",
    "            pos_v2 = view2[topk_v2.indices[:, k]]\n",
    "            anchors_v1 = view1[indices]\n",
    "            anchors_v2 = view2[indices]\n",
    "            \n",
    "            loss += 0.5 * InfoNCE(anchors_v1, pos_v2, self.tau)\n",
    "            loss += 0.5 * InfoNCE(anchors_v2, pos_v1, self.tau)\n",
    "        \n",
    "        return loss / (batch_size * (self.local_k + 1))\n",
    "    \n",
    "    def cal_enhanced_msbe_loss(self, independent_indices, neighbor_dict, view1, view2):\n",
    "        loss = 0.0\n",
    "        nodes_cpu = independent_indices.cpu().tolist()\n",
    "        anchors, positives = [], []\n",
    "        \n",
    "        for idx in nodes_cpu:\n",
    "            if idx in neighbor_dict and len(neighbor_dict[idx]) > 0:\n",
    "                neighbors = neighbor_dict[idx]\n",
    "                k = min(self.top_k, len(neighbors))\n",
    "                sampled_neighbors = random.sample(neighbors, k)\n",
    "                \n",
    "                # Aggregate Neighbor Representations\n",
    "                neighbor_embs = view2[sampled_neighbors] # Use other view\n",
    "                pos_emb = torch.mean(neighbor_embs, dim=0)\n",
    "                \n",
    "                # Robustness: Add noise (Stochastic Structure)\n",
    "                if self.training and random.random() > 0.5:\n",
    "                    noise = torch.randn_like(pos_emb) * 0.01\n",
    "                    pos_emb = F.normalize(pos_emb + noise, dim=-1)\n",
    "                    \n",
    "                anchors.append(view1[idx])\n",
    "                positives.append(pos_emb)\n",
    "        \n",
    "        if len(anchors) > 0:\n",
    "            anchors = torch.stack(anchors)\n",
    "            positives = torch.stack(positives)\n",
    "            loss = InfoNCE(anchors, positives, self.tau)\n",
    "            \n",
    "            # Additional Hard Negative Penalty\n",
    "            if len(anchors) > 1:\n",
    "                sim_matrix = torch.matmul(anchors, positives.T)\n",
    "                mask = torch.eye(len(anchors), device=anchors.device).bool()\n",
    "                sim_matrix = sim_matrix.masked_fill(mask, -1e9)\n",
    "                hardest_neg = torch.max(sim_matrix, dim=1)[0]\n",
    "                loss += 0.1 * torch.mean(hardest_neg)\n",
    "                \n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        \n",
    "        for epoch in range(self.maxEpoch):\n",
    "            # Dynamic Weights\n",
    "            if self.use_dynamic_weight:\n",
    "                warmup_ratio = min(1.0, (epoch + 1) / self.warmup_epochs)\n",
    "                cl_rate = self.base_cl_rate * warmup_ratio\n",
    "                msb_rate = self.base_msb_rate * warmup_ratio\n",
    "                local_rate = self.local_rate * warmup_ratio\n",
    "            else:\n",
    "                cl_rate, msb_rate, local_rate = self.base_cl_rate, self.base_msb_rate, self.local_rate\n",
    "                \n",
    "            epoch_rec_loss = epoch_cl_loss = epoch_msb_loss = epoch_local_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                user_idx, pos_idx, neg_idx = batch\n",
    "                \n",
    "                # 1. Main Rec (Clean)\n",
    "                rec_user_emb, rec_item_emb = model(perturbed=False)\n",
    "                user_emb = rec_user_emb[user_idx]\n",
    "                pos_item_emb = rec_item_emb[pos_idx]\n",
    "                neg_item_emb = rec_item_emb[neg_idx]\n",
    "                l_rec = bpr_loss(user_emb, pos_item_emb, neg_item_emb)\n",
    "                \n",
    "                # 2. Augmentation Views\n",
    "                user_v1, item_v1 = model(perturbed=True)\n",
    "                user_v2, item_v2 = model(perturbed=True)\n",
    "                \n",
    "                u_uniq = torch.unique(torch.tensor(user_idx).cuda())\n",
    "                i_uniq = torch.unique(torch.tensor(pos_idx).cuda())\n",
    "                \n",
    "                # 3. Uniformity (SimGCL)\n",
    "                l_uniform = InfoNCE(user_v1[u_uniq], user_v2[u_uniq], self.tau) + \\\n",
    "                            InfoNCE(item_v1[i_uniq], item_v2[i_uniq], self.tau)\n",
    "                            \n",
    "                # 4. Structural Loss (MSBE Enhanced)\n",
    "                l_struct = self.cal_enhanced_msbe_loss(u_uniq, self.user_msb_neighbors, user_v1, user_v2) + \\\n",
    "                           self.cal_enhanced_msbe_loss(i_uniq, self.item_msb_neighbors, item_v1, item_v2)\n",
    "                           \n",
    "                # 5. Local Similarity (GLSCL Style)\n",
    "                l_local = self.local_sim_loss(user_v1, user_v2, u_uniq, self.user_msb_neighbors, 'user') + \\\n",
    "                          self.local_sim_loss(item_v1, item_v2, i_uniq, self.item_msb_neighbors, 'item')\n",
    "                \n",
    "                # Total Loss\n",
    "                total_loss = l_rec + \\\n",
    "                             cl_rate * l_uniform + \\\n",
    "                             msb_rate * l_struct + \\\n",
    "                             local_rate * l_local + \\\n",
    "                             l2_reg_loss(self.reg, user_emb, pos_item_emb, neg_item_emb)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_rec_loss += l_rec.item()\n",
    "                epoch_cl_loss += l_uniform.item()\n",
    "                epoch_msb_loss += l_struct.item()\n",
    "                epoch_local_loss += l_local.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                if n % 100 == 0 and n > 0:\n",
    "                    print(f'Epoch {epoch} Batch {n}: Rec={l_rec.item():.4f} Uni={l_uniform.item():.4f} Str={l_struct.item():.4f} Loc={l_local.item():.4f} (G={msb_rate:.3f})')\n",
    "                    \n",
    "            # Evaluation\n",
    "            print(f'\\nEpoch {epoch} Avg: Rec={epoch_rec_loss/batch_count:.4f} Uni={epoch_cl_loss/batch_count:.4f} Str={epoch_msb_loss/batch_count:.4f} Loc={epoch_local_loss/batch_count:.4f}')\n",
    "            with torch.no_grad():\n",
    "                self.user_emb, self.item_emb = self.model(perturbed=False)\n",
    "            self.fast_evaluation(epoch)\n",
    "        self.user_emb, self.item_emb = self.best_user_emb, self.best_item_emb\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_user_emb, self.best_item_emb = self.model(perturbed=False)\n",
    "\n",
    "    def predict(self, u):\n",
    "        u = self.data.get_user_id(u)\n",
    "        score = torch.matmul(self.user_emb[u], self.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "class MSBEGCL_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, eps, n_layers):\n",
    "        super(MSBEGCL_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.eps = eps\n",
    "        self.emb_size = emb_size\n",
    "        self.n_layers = n_layers\n",
    "        self.sparse_norm_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(data.norm_adj).cuda()\n",
    "        self.embedding_dict = self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.data.user_num, self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.data.item_num, self.emb_size))),\n",
    "        })\n",
    "        return embedding_dict\n",
    "\n",
    "    def forward(self, perturbed=False):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = []\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(self.sparse_norm_adj, ego_embeddings)\n",
    "            if perturbed:\n",
    "                random_noise = torch.rand_like(ego_embeddings).cuda()\n",
    "                ego_embeddings += torch.sign(ego_embeddings) * F.normalize(random_noise, dim=-1) * self.eps\n",
    "            all_embeddings.append(ego_embeddings)\n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        all_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        user_all_embeddings, item_all_embeddings = torch.split(all_embeddings, [self.data.user_num, self.data.item_num])\n",
    "        return user_all_embeddings, item_all_embeddings\n",
    "'''\n",
    "model_path = os.path.join(selfrec_path, 'model', 'graph', 'MSBEGCL.py')\n",
    "with open(model_path, 'w') as f:\n",
    "    f.write(model_code)\n",
    "print(\"Patched model/graph/MSBEGCL.py\")\n",
    "\n",
    "# 9. Run MSBEGCL\n",
    "print('\\n--- Starting Training ---')\n",
    "\n",
    "main_py_path = os.path.join(selfrec_path, 'main.py')\n",
    "if not os.path.exists(main_py_path):\n",
    "    print(f\"CRITICAL: {main_py_path} not found.\")\n",
    "\n",
    "os.chdir(selfrec_path)\n",
    "print(f\"Changed directory to {os.getcwd()} for training.\")\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [sys.executable, '-u', 'main.py'],\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT, \n",
    "    text=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "try:\n",
    "    process.stdin.write(f'{model_name}\\n')\n",
    "    process.stdin.flush()\n",
    "    process.stdin.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to stdin: {e}\")\n",
    "\n",
    "while True:\n",
    "    line = process.stdout.readline()\n",
    "    if not line and process.poll() is not None:\n",
    "        break\n",
    "    if line:\n",
    "        l = line.strip()\n",
    "        keywords = [\"training:\", \"Ranking Performance\", \"*Current Performance*\", \"*Best Performance*\", \"Epoch:\", \"Hit Ratio\", \"Traceback\", \"Error\", \"File \\\"\"]\n",
    "        if any(k in l for k in keywords):\n",
    "            print(l)\n",
    "\n",
    "if process.poll() != 0:\n",
    "    print(\"Training failed.\")\n",
    "else:\n",
    "    print(\"Training finished successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
