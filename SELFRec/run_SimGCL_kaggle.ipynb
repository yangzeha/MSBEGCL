{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d690e21",
   "metadata": {},
   "source": [
    "# Run MSBEGCL on Kaggle\n",
    "\n",
    "This notebook sets up the environment, compiles the necessary C++ mining tools, prepares the data, runs the mining algorithm to generate bicliques, and finally trains the MSBEGCL recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, time, shutil, struct, re, itertools, math\n",
    "\n",
    "# --- Configuration ---\n",
    "repo_url = 'https://github.com/yangzeha/MSBEGCL.git'\n",
    "repo_dir = 'MSBEGCL'\n",
    "model_name = 'MSBEGCL'\n",
    "dataset_name = 'yelp2018'\n",
    "\n",
    "# 1. Clean and Clone Repository (Silent)\n",
    "if os.path.exists(repo_dir):\n",
    "    try:\n",
    "        shutil.rmtree(repo_dir)\n",
    "    except Exception as e:\n",
    "        subprocess.run(['rm', '-rf', repo_dir], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "try:\n",
    "    subprocess.run(['git', 'clone', '-b', 'master', repo_url], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    sys.exit(1)\n",
    "\n",
    "# 2. Setup Directories (Silent)\n",
    "if os.path.basename(os.getcwd()) != repo_dir:\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "# [Robustness Fix]: Auto-detect nested structure\n",
    "roots = os.listdir('.')\n",
    "target_structure_found = False\n",
    "possible_subdirs = ['.', 'MSBEGCL', 'msbegcl', repo_dir]\n",
    "\n",
    "for d in possible_subdirs:\n",
    "    if d == '.': path_to_check = '.'\n",
    "    else:\n",
    "        path_to_check = d\n",
    "        if not os.path.exists(d) or not os.path.isdir(d): continue\n",
    "    contents = os.listdir(path_to_check)\n",
    "    if 'SELFRec' in contents and 'Similar-Biclique-Idx' in contents:\n",
    "        if d != '.': os.chdir(d)\n",
    "        target_structure_found = True\n",
    "        break\n",
    "\n",
    "if not target_structure_found:\n",
    "    found = False\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        if 'SELFRec' in dirs:\n",
    "            os.chdir(root)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(\"CRITICAL ERROR: Could not locate SELFRec directory anywhere.\")\n",
    "\n",
    "selfrec_path = 'SELFRec'\n",
    "msbe_path = 'Similar-Biclique-Idx'\n",
    "\n",
    "# 3. Install Dependencies (Silent)\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'PyYAML==6.0.2', 'scipy==1.14.1', '-q'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'faiss-cpu', '-q'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# [CRITICAL FIX]: Ensure util package integrity and alias config\n",
    "print(\"--- Fixing Import/Module Errors ---\")\n",
    "util_dir = os.path.join(selfrec_path, 'util')\n",
    "if os.path.exists(util_dir):\n",
    "    # Ensure __init__.py exists\n",
    "    if not os.path.exists(os.path.join(util_dir, '__init__.py')):\n",
    "        with open(os.path.join(util_dir, '__init__.py'), 'w') as f: f.write('')\n",
    "    \n",
    "    # Alias conf.py to config.py to satisfy any legacy/broken imports\n",
    "    conf_file = os.path.join(util_dir, 'conf.py')\n",
    "    config_file = os.path.join(util_dir, 'config.py')\n",
    "    if os.path.exists(conf_file) and not os.path.exists(config_file):\n",
    "        shutil.copy(conf_file, config_file)\n",
    "        print(\"   [Fix] Aliased util/conf.py -> util/config.py\")\n",
    "\n",
    "# 4. Compile C++ Mining Tools (Silent)\n",
    "sparsez_dir = 'sparsehash'\n",
    "if not os.path.exists(sparsez_dir):\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/sparsehash/sparsehash.git'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    cwd_backup = os.getcwd()\n",
    "    os.chdir(sparsez_dir)\n",
    "    try:\n",
    "        subprocess.run(['chmod', '+x', 'configure'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) \n",
    "        subprocess.run(['./configure'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        subprocess.run(['make'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    finally:\n",
    "        os.chdir(cwd_backup)\n",
    "\n",
    "# Compile msbe\n",
    "msbe_src = os.path.join(msbe_path, 'main.cpp')\n",
    "msbe_exe = './msbe'\n",
    "if not os.path.exists(msbe_src):\n",
    "    print(f\"CRITICAL ERROR: Source file {msbe_src} not found!\")\n",
    "else:\n",
    "    subprocess.run(['g++', '-w', '-O3', msbe_src, '-o', msbe_exe, '-I', msbe_path, '-I', 'sparsehash/src', '-D_PrintResults_', '-D_CheckResults_'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    subprocess.run(['chmod', '+x', msbe_exe], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# 5. Data Preprocessing\n",
    "print(f'\\n--- Preprocessing {dataset_name} for Mining ---')\n",
    "train_file = os.path.join(selfrec_path, 'dataset', dataset_name, 'train.txt')\n",
    "mining_graph_txt = 'graph.txt'\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print(f\"CRITICAL ERROR: Data file {train_file} not found!\")\n",
    "else:\n",
    "    users = set()\n",
    "    items = set()\n",
    "    edges = []\n",
    "    with open(train_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                u, i = parts[0], parts[1]\n",
    "                users.add(u)\n",
    "                items.add(i)\n",
    "                edges.append((u, i))\n",
    "\n",
    "    try:\n",
    "        sorted_users = sorted(list(users), key=lambda x: int(x))\n",
    "        sorted_items = sorted(list(items), key=lambda x: int(x))\n",
    "    except:\n",
    "        sorted_users = sorted(list(users))\n",
    "        sorted_items = sorted(list(items))\n",
    "\n",
    "    u_map = {u: idx for idx, u in enumerate(sorted_users)}\n",
    "    i_map = {i: idx for idx, i in enumerate(sorted_items)}\n",
    "\n",
    "    n1 = len(users)\n",
    "    n2 = len(items)\n",
    "   \n",
    "    print(f'Preprocessing graph with {n1} users, {n2} items, {len(edges)} edges.')\n",
    "    \n",
    "    total_nodes = n1 + n2\n",
    "    adj = [[] for _ in range(total_nodes)]\n",
    "    edge_count = 0\n",
    "    \n",
    "    for u, i in edges:\n",
    "        uid = u_map[u]\n",
    "        iid = i_map[i] + n1\n",
    "        adj[uid].append(iid)\n",
    "        adj[iid].append(uid)\n",
    "        edge_count += 2\n",
    "        \n",
    "    for k in range(total_nodes):\n",
    "        adj[k].sort()\n",
    "        \n",
    "    degree_file = 'graph_b_degree.bin'\n",
    "    with open(degree_file, 'wb') as f:\n",
    "        f.write(struct.pack('I', 4))\n",
    "        f.write(struct.pack('I', n1))\n",
    "        f.write(struct.pack('I', n2))\n",
    "        f.write(struct.pack('I', edge_count))\n",
    "        degrees = [len(adj[k]) for k in range(total_nodes)]\n",
    "        f.write(struct.pack(f'{total_nodes}I', *degrees))\n",
    "        \n",
    "    adj_file = 'graph_b_adj.bin'\n",
    "    with open(adj_file, 'wb') as f:\n",
    "        flat_adj = []\n",
    "        for k in range(total_nodes):\n",
    "            flat_adj.extend(adj[k])\n",
    "        f.write(struct.pack(f'{edge_count}I', *flat_adj))\n",
    "        \n",
    "    print(f\"Generated binary graph files.\")\n",
    "    \n",
    "    with open(mining_graph_txt, 'w') as f:\n",
    "        f.write(\"dummy\")\n",
    "\n",
    "# 6. Run Mining\n",
    "print('\\n--- Mining Bicliques (Structure Discovery) ---')\n",
    "# [Strategy]: Use relaxed threshold to ensure high recall of structural candidates\n",
    "sim_threshold = 0.2\n",
    "size_threshold = 2\n",
    "\n",
    "if os.path.exists(msbe_exe) and os.path.exists(mining_graph_txt):\n",
    "    print('Building Index...')\n",
    "    subprocess.run([msbe_exe, mining_graph_txt, '1', '1', str(sim_threshold), 'GRL3'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    print('Enumerating...')\n",
    "    raw_bicliques_file = 'bicliques_raw.txt'\n",
    "    with open(raw_bicliques_file, 'w') as outfile:\n",
    "        subprocess.run([\n",
    "            msbe_exe, mining_graph_txt, \n",
    "            '0', '1', str(sim_threshold), 'GRL3', \n",
    "            '1', 'GRL3', \n",
    "            '0', '0', 'heu', \n",
    "            '4', str(sim_threshold), str(size_threshold), '2'\n",
    "        ], stdout=outfile, stderr=subprocess.DEVNULL, check=True)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping mining due to compliation or data failure.\")\n",
    "\n",
    "# 7. Process Bicliques -> Model Format\n",
    "print('\\n--- Formatting Bicliques for Model ---')\n",
    "final_biclique_path = os.path.join(selfrec_path, 'dataset', dataset_name, 'bicliques.txt')\n",
    "count = 0\n",
    "\n",
    "if os.path.exists(raw_bicliques_file):\n",
    "    with open(raw_bicliques_file, 'r') as fr, open(final_biclique_path, 'w') as fw:\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            clean_line = line.replace('|', ' ').replace(',', ' ').replace(':', ' ')\n",
    "            tokens = clean_line.split()\n",
    "            current_users = []\n",
    "            current_items = []\n",
    "            for t in tokens:\n",
    "                if not t.isdigit(): continue\n",
    "                nid = int(t)\n",
    "                if nid < n1:\n",
    "                    if nid < len(sorted_users):\n",
    "                        current_users.append(sorted_users[nid])\n",
    "                else:\n",
    "                    iid = nid - n1\n",
    "                    if iid >= 0 and iid < len(sorted_items):\n",
    "                        current_items.append(sorted_items[iid])\n",
    "            if len(current_users) > 0 and len(current_items) > 0:\n",
    "                fw.write(f\"{' '.join(current_users)} | {' '.join(current_items)}\\n\")\n",
    "                count += 1\n",
    "    print(f\"Processed {count} bicliques into {final_biclique_path}\")\n",
    "    \n",
    "# [Patching] Write Helper Files\n",
    "print('--- Patching Model Code in Notebook ---')\n",
    "denoising_code = r'''\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def filter_large_bicliques(bicliques, max_size=30):\n",
    "    filtered = []\n",
    "    for users, items in bicliques:\n",
    "        if len(users) <= max_size and len(items) <= max_size:\n",
    "            filtered.append((users, items))\n",
    "    return filtered\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def prune_low_similarity_clusters(bicliques, interaction_mat, min_sim=0.2):\n",
    "    pruned = []\n",
    "    for users, items in bicliques:\n",
    "        user_sims = []\n",
    "        user_list = list(users)\n",
    "        for i in range(len(user_list)):\n",
    "            for j in range(i+1, len(user_list)):\n",
    "                try:\n",
    "                    u1_idx = interaction_mat[user_list[i]].indices\n",
    "                    u2_idx = interaction_mat[user_list[j]].indices\n",
    "                except:\n",
    "                    try:\n",
    "                         u1_idx = np.where(interaction_mat[user_list[i]] > 0)[0]\n",
    "                         u2_idx = np.where(interaction_mat[user_list[j]] > 0)[0]\n",
    "                    except:\n",
    "                        continue\n",
    "                u1_interactions = set(u1_idx)\n",
    "                u2_interactions = set(u2_idx)\n",
    "                sim = compute_jaccard_similarity(u1_interactions, u2_interactions)\n",
    "                user_sims.append(sim)\n",
    "        avg_sim = np.mean(user_sims) if user_sims else 0\n",
    "        if avg_sim >= min_sim:\n",
    "            pruned.append((users, items))\n",
    "    return pruned\n",
    "\n",
    "def build_enhanced_neighbor_dict(bicliques, user_map, item_map, top_k=10):\n",
    "    user_neighbors = defaultdict(list)\n",
    "    item_neighbors = defaultdict(list)\n",
    "    for users, items in bicliques:\n",
    "        user_ids = [user_map[u] for u in users if u in user_map]\n",
    "        item_ids = [item_map[i] for i in items if i in item_map]\n",
    "        for u in user_ids:\n",
    "            for other_u in user_ids:\n",
    "                if u != other_u: user_neighbors[u].append(other_u)\n",
    "        for i in item_ids:\n",
    "            for other_i in item_ids:\n",
    "                if i != other_i: item_neighbors[i].append(other_i)\n",
    "    enhanced_user = {}\n",
    "    enhanced_item = {}\n",
    "    for u, neighbors in user_neighbors.items():\n",
    "        unique_neighbors = list(set(neighbors))\n",
    "        if len(unique_neighbors) > top_k:\n",
    "            enhanced_user[u] = random.sample(unique_neighbors, top_k)\n",
    "        else:\n",
    "            enhanced_user[u] = unique_neighbors\n",
    "    for i, neighbors in item_neighbors.items():\n",
    "        unique_neighbors = list(set(neighbors))\n",
    "        if len(unique_neighbors) > top_k:\n",
    "            enhanced_item[i] = random.sample(unique_neighbors, top_k)\n",
    "        else:\n",
    "            enhanced_item[i] = unique_neighbors\n",
    "    return enhanced_user, enhanced_item\n",
    "'''\n",
    "denoise_path = os.path.join(selfrec_path, 'util', 'denoising_helper.py')\n",
    "os.makedirs(os.path.dirname(denoise_path), exist_ok=True)\n",
    "with open(denoise_path, 'w') as f:\n",
    "    f.write(denoising_code)\n",
    "\n",
    "msbe_helper_code = r'''\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from util.denoising_helper import filter_large_bicliques, prune_low_similarity_clusters, build_enhanced_neighbor_dict\n",
    "\n",
    "def load_msbe_neighbors(file_path, user_map, item_map, interaction_mat=None, sim_threshold=0.2):\n",
    "    user_neighbors = {}\n",
    "    item_neighbors = {}\n",
    "    print(f\"Loading bicliques from {file_path}, sim_threshold={sim_threshold}\")\n",
    "    if os.path.exists(file_path):\n",
    "        bicliques = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) < 2: continue\n",
    "                users = [u.strip() for u in parts[0].split() if u.strip() in user_map]\n",
    "                items = [i.strip() for i in parts[1].split() if i.strip() in item_map]\n",
    "                if len(users) > 0 and len(items) > 0:\n",
    "                    bicliques.append((users, items))\n",
    "        print(f\"Original bicliques: {len(bicliques)}\")\n",
    "        filtered = filter_large_bicliques(bicliques, max_size=30)\n",
    "        if interaction_mat is not None:\n",
    "            try:\n",
    "                pruned = prune_low_similarity_clusters(filtered, interaction_mat, min_sim=sim_threshold)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Pruning failed ({e}), using filtered only.\")\n",
    "                pruned = filtered\n",
    "            print(f\"Pruned bicliques: {len(pruned)}\")\n",
    "        else:\n",
    "            pruned = filtered\n",
    "        user_neighbors, item_neighbors = build_enhanced_neighbor_dict(pruned, user_map, item_map, top_k=15)\n",
    "        u_c = sum(1 for v in user_neighbors.values() if len(v) > 0)\n",
    "        i_c = sum(1 for v in item_neighbors.values() if len(v) > 0)\n",
    "        print(f\"Enhanced Neighbors: {u_c} users, {i_c} items.\")\n",
    "    return user_neighbors, item_neighbors\n",
    "'''\n",
    "helper_path = os.path.join(selfrec_path, 'util', 'msbe_helper.py')\n",
    "os.makedirs(os.path.dirname(helper_path), exist_ok=True)\n",
    "with open(helper_path, 'w') as f:\n",
    "    f.write(msbe_helper_code)\n",
    "\n",
    "\n",
    "# 3. MSBEGCL.py (Rocket Strategy + Innovation 3: Hard Negative Mining)\n",
    "# [Robustness Fix]: Added device agnostic code (CPU/CUDA check) to prevent crashes on non-GPU envs\n",
    "# [Hotfix]: Added missing 'import os'\n",
    "model_code = r'''\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss\n",
    "import random\n",
    "import numpy as np\n",
    "import traceback\n",
    "from util.msbe_helper import load_msbe_neighbors\n",
    "\n",
    "class MSBEGCL(GraphRecommender):\n",
    "    def __init__(self, conf, training_set, test_set):\n",
    "        super(MSBEGCL, self).__init__(conf, training_set, test_set)\n",
    "        \n",
    "        # [Device Safe]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # [Robustness Fix]: Use safe check instead of 'in' operator to avoid ModelConf __getitem__ iteration bug\n",
    "        if not self.config.contain('MSBEGCL'):\n",
    "            print(\"WARNING: MSBEGCL config section not found! Using hardcoded defaults.\")\n",
    "            args = {'n_layer': 2, 'lambda': 0.5, 'gamma': 0.05, 'eps': 0.1, 'tau': 0.2, 'biclique.file': ''}\n",
    "        else:\n",
    "            args = self.config['MSBEGCL']\n",
    "            \n",
    "        self.n_layers = int(args.get('n_layer', 2))\n",
    "        self.eps = float(args.get('eps', 0.1))\n",
    "        self.tau = float(args.get('tau', 0.2))\n",
    "        \n",
    "        self.cl_rate = float(args.get('lambda', 0.5))\n",
    "        self.msbe_rate = float(args.get('gamma', 0.05))\n",
    "        \n",
    "        # [Innovation 3]: Hard Negative Weight\n",
    "        self.hard_weight = float(args.get('hard_weight', 2.0))\n",
    "        \n",
    "        print(f\"MSBEGCL Init: lambda={self.cl_rate}, gamma={self.msbe_rate}, tau={self.tau}, hard_w={self.hard_weight}, device={self.device}\")\n",
    "        \n",
    "        self.model = MSBEGCL_Encoder(self.data, self.emb_size, self.eps, self.n_layers, self.device)\n",
    "        \n",
    "        self.biclique_file = args.get('biclique.file', '')\n",
    "        self.sim_threshold = float(args.get('sim_threshold', 0.2))\n",
    "        \n",
    "        if os.path.exists(self.biclique_file):\n",
    "            self.user_msb, self.item_msb = self.load_denoised_neighbors()\n",
    "        else:\n",
    "            print(f\"Warning: Biclique file {self.biclique_file} not found. Running w/o structure.\")\n",
    "            self.user_msb, self.item_msb = {}, {}\n",
    "        \n",
    "    def load_denoised_neighbors(self):\n",
    "        user_neighbors, item_neighbors = load_msbe_neighbors(\n",
    "            self.biclique_file,\n",
    "            self.data.user,\n",
    "            self.data.item,\n",
    "            self.data.interaction_mat, \n",
    "            self.sim_threshold\n",
    "        )\n",
    "        return user_neighbors, item_neighbors\n",
    "\n",
    "    # [Innovation 3]: Structure-Aware Hard Negative InfoNCE\n",
    "    def info_nce_with_hard_neg(self, view1, view2, nodes, neighbor_dict, full_embeddings):\n",
    "        \"\"\"\n",
    "        Structure-Aware Hard Negative Mining:\n",
    "        Includes structurally similar nodes (from Bicliques) as heavily weighted negatives\n",
    "        in the contrastive loss denominator.\n",
    "        \"\"\"\n",
    "        # Normalize inputs for Cosine Similarity\n",
    "        view1 = F.normalize(view1, dim=1)\n",
    "        view2 = F.normalize(view2, dim=1)\n",
    "        \n",
    "        # 1. Positive Pairs (Diagonal)\n",
    "        pos_score = torch.sum(view1 * view2, dim=1)\n",
    "        exp_pos = torch.exp(pos_score / self.tau)\n",
    "        \n",
    "        # 2. Standard Batch Negatives (SimGCL default)\n",
    "        # Similarity between view1 and ALL view2s in batch\n",
    "        sim_all = torch.matmul(view1, view2.t())\n",
    "        exp_all = torch.exp(sim_all / self.tau).sum(dim=1)\n",
    "        \n",
    "        # 3. Structure-Aware Hard Negatives\n",
    "        # For each node, pick a 'Hard Negative' from its biclique neighbors\n",
    "        if self.hard_weight > 0.0:\n",
    "            nodes_np = nodes.cpu().numpy()\n",
    "            hard_indices = []\n",
    "            mask_list = []\n",
    "            \n",
    "            for nid in nodes_np:\n",
    "                neighbors = neighbor_dict.get(nid, [])\n",
    "                if neighbors:\n",
    "                    # Pick random structural neighbor as hard negative\n",
    "                    hard_idx = random.choice(neighbors)\n",
    "                    hard_indices.append(hard_idx)\n",
    "                    mask_list.append(1.0)\n",
    "                else:\n",
    "                    hard_indices.append(nid) # Dummy (masked out)\n",
    "                    mask_list.append(0.0)\n",
    "            \n",
    "            hard_tensor = torch.tensor(hard_indices, device=view1.device, dtype=torch.long)\n",
    "            mask = torch.tensor(mask_list, device=view1.device)\n",
    "            \n",
    "            # Lookup embedding for hard negative (using main view approximation)\n",
    "            hard_embs = full_embeddings[hard_tensor]\n",
    "            hard_embs = F.normalize(hard_embs, dim=1)\n",
    "            \n",
    "            # Compute similarity with hard negative\n",
    "            hard_score = torch.sum(view1 * hard_embs, dim=1)\n",
    "            exp_hard = torch.exp(hard_score / self.tau)\n",
    "            \n",
    "            # Add to denominator with extra weight\n",
    "            denominator = exp_all + (self.hard_weight * exp_hard * mask)\n",
    "        else:\n",
    "            denominator = exp_all\n",
    "            \n",
    "        loss = -torch.log(exp_pos / denominator)\n",
    "        return loss.mean()\n",
    "\n",
    "    def cal_msbe_loss(self, nodes, neighbors_dict, embeddings):\n",
    "        \"\"\"\n",
    "        Parallel Dual-View Injection (Innovation 1/2)\n",
    "        \"\"\"\n",
    "        if len(nodes) == 0: return torch.tensor(0.0, device=embeddings.device)\n",
    "        \n",
    "        nodes_list = nodes.cpu().tolist()\n",
    "        struct_targets = []\n",
    "        valid_sources = []\n",
    "        \n",
    "        for node in nodes_list:\n",
    "            if node in neighbors_dict and len(neighbors_dict[node]) > 1:\n",
    "                m_list = neighbors_dict[node]\n",
    "                sample_size = min(len(m_list), 10)\n",
    "                sampled = random.sample(m_list, sample_size)\n",
    "                \n",
    "                neighbor_embs = embeddings[sampled]\n",
    "                center = torch.mean(neighbor_embs.detach(), dim=0)\n",
    "                \n",
    "                struct_targets.append(center)\n",
    "                valid_sources.append(embeddings[node])\n",
    "\n",
    "        if len(valid_sources) == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device)\n",
    "            \n",
    "        sources = torch.stack(valid_sources)\n",
    "        targets = torch.stack(struct_targets)\n",
    "        \n",
    "        # Standard InfoNCE (or MSE) for Injection, keep it sharp\n",
    "        sources = F.normalize(sources, dim=1)\n",
    "        targets = F.normalize(targets, dim=1)\n",
    "        sim = torch.sum(sources * targets, dim=1)\n",
    "        return -torch.log(torch.exp(sim / 0.1)).mean()\n",
    "\n",
    "    def train(self):\n",
    "        model = self.model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lRate)\n",
    "        \n",
    "        for epoch in range(self.maxEpoch):\n",
    "            try:\n",
    "                current_gamma = self.msbe_rate \n",
    "                \n",
    "                epoch_rec = 0\n",
    "                epoch_cl = 0\n",
    "                epoch_msb = 0\n",
    "                batch_c = 0\n",
    "                \n",
    "                for n, batch in enumerate(next_batch_pairwise(self.data, self.batch_size)):\n",
    "                    user_idx, pos_idx, neg_idx = batch\n",
    "                    \n",
    "                    # Augmentation Views\n",
    "                    u_v1, i_v1 = model(perturbed=True)\n",
    "                    u_v2, i_v2 = model(perturbed=True)\n",
    "                    \n",
    "                    # Main View (Shared by Rec, MSBE, and Hard Neg Lookup)\n",
    "                    res_u, res_i = model(perturbed=False)\n",
    "                    \n",
    "                    # 1. Rec Loss\n",
    "                    l_rec = bpr_loss(res_u[user_idx], res_i[pos_idx], res_i[neg_idx]) + \\\n",
    "                            l2_reg_loss(self.reg, res_u[user_idx], res_i[pos_idx], res_i[neg_idx])\n",
    "                    \n",
    "                    # 2. Structure-Aware Contrastive Loss (Innovation 3)\n",
    "                    u_uniq = torch.unique(torch.tensor(user_idx).to(self.device))\n",
    "                    i_uniq = torch.unique(torch.tensor(pos_idx).to(self.device))\n",
    "                    \n",
    "                    # Pass main embeddings (res_u, res_i) to lookup hard negatives\n",
    "                    cl_u = self.info_nce_with_hard_neg(u_v1[u_uniq], u_v2[u_uniq], u_uniq, self.user_msb, res_u)\n",
    "                    cl_i = self.info_nce_with_hard_neg(i_v1[i_uniq], i_v2[i_uniq], i_uniq, self.item_msb, res_i)\n",
    "                    \n",
    "                    l_sim = self.cl_rate * (cl_u + cl_i)\n",
    "                                \n",
    "                    # 3. Parallel Injection Loss (Innovation 1/2)\n",
    "                    l_msbe = current_gamma * (\n",
    "                        self.cal_msbe_loss(u_uniq, self.user_msb, res_u) +\n",
    "                        self.cal_msbe_loss(i_uniq, self.item_msb, res_i)\n",
    "                    )\n",
    "                    \n",
    "                    total_loss = l_rec + l_sim + l_msbe\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_rec += l_rec.item()\n",
    "                    epoch_cl += l_sim.item()\n",
    "                    epoch_msb += l_msbe.item()\n",
    "                    batch_c += 1\n",
    "                \n",
    "                print(f'Epoch {epoch} Avg: Rec={epoch_rec/batch_c:.4f} Sim={epoch_cl/batch_c:.4f} Msb={epoch_msb/batch_c:.4f}')\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    self.user_emb, self.item_emb = self.model(perturbed=False)\n",
    "                self.fast_evaluation(epoch)\n",
    "\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "\n",
    "    def save(self):\n",
    "        with torch.no_grad():\n",
    "            self.best_user_emb, self.best_item_emb = self.model(perturbed=False)\n",
    "\n",
    "    def predict(self, u):\n",
    "        u = self.data.get_user_id(u)\n",
    "        score = torch.matmul(self.user_emb[u], self.item_emb.transpose(0, 1))\n",
    "        return score.cpu().numpy()\n",
    "\n",
    "class MSBEGCL_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, eps, n_layers, device):\n",
    "        super(MSBEGCL_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.eps = eps\n",
    "        self.emb_size = emb_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.sparse_norm_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(data.norm_adj).to(self.device)\n",
    "        self.embedding_dict = self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        # Using Xavier Init for stability\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.data.user_num, self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.data.item_num, self.emb_size))),\n",
    "        })\n",
    "        return embedding_dict\n",
    "\n",
    "    def forward(self, perturbed=False):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = []\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(self.sparse_norm_adj, ego_embeddings)\n",
    "            if perturbed:\n",
    "                random_noise = torch.rand_like(ego_embeddings).to(self.device)\n",
    "                ego_embeddings += torch.sign(ego_embeddings) * F.normalize(random_noise, dim=-1) * self.eps\n",
    "            all_embeddings.append(ego_embeddings)\n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        all_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        user_all_embeddings, item_all_embeddings = torch.split(all_embeddings, [self.data.user_num, self.data.item_num])\n",
    "        return user_all_embeddings, item_all_embeddings\n",
    "'''\n",
    "model_path = os.path.join(selfrec_path, 'model', 'graph', 'MSBEGCL.py')\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "with open(model_path, 'w') as f:\n",
    "    f.write(model_code)\n",
    "print(\"Patched model/graph/MSBEGCL.py with Structure-Aware Hard Negative Mining\")\n",
    "\n",
    "# --- 8. Auto-Tuner Loop ---\n",
    "print('\\n>>> STARTING AUTO-TUNER SEARCH (Target: Epoch 0/1 Recall@20 > 0.065) <<<')\n",
    "\n",
    "# Search Space\n",
    "lambdas = [0.1, 0.2, 0.5]\n",
    "gammas = [0.01, 0.05, 0.1, 0.2]\n",
    "taus = [0.1, 0.2]\n",
    "\n",
    "results_log = []\n",
    "found_target = False\n",
    "\n",
    "os.chdir(selfrec_path)\n",
    "os.makedirs('results', exist_ok=True) # Ensure output dir exists\n",
    "\n",
    "for lam, gam, tau in itertools.product(lambdas, gammas, taus):\n",
    "    print(f\"\\n──────────────────────────────────────────────\")\n",
    "    print(f\"Testing Config: lambda={lam}, gamma={gam}, tau={tau}\")\n",
    "    print(f\"──────────────────────────────────────────────\")\n",
    "    \n",
    "    # 1. Generate Config\n",
    "    clean_biclique_path = f'./dataset/{dataset_name}/bicliques.txt'\n",
    "    yaml_content = f\"\"\"\n",
    "training.set: ./dataset/{dataset_name}/train.txt\n",
    "test.set: ./dataset/{dataset_name}/test.txt\n",
    "output: ./results\n",
    "\n",
    "model:\n",
    "  name: MSBEGCL\n",
    "  type: graph\n",
    "\n",
    "item.ranking.topN: [10,20]\n",
    "embedding.size: 64\n",
    "max.epoch: 2\n",
    "batch.size: 4096\n",
    "learning.rate: 0.001\n",
    "reg.lambda: 0.0001\n",
    "output: ./results\n",
    "\n",
    "MSBEGCL:\n",
    "  n_layer: 2\n",
    "  lambda: {lam}\n",
    "  gamma: {gam}\n",
    "  eps: 0.1\n",
    "  tau: {tau}\n",
    "  hard_weight: 2.0  # [Innovation 3]: Enable Hard Negative Mining\n",
    "  biclique.file: {clean_biclique_path}\n",
    "  sim_threshold: {sim_threshold}\n",
    "\"\"\"\n",
    "    # Write config\n",
    "    with open('conf/MSBEGCL.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "        \n",
    "    # 2. Run Process\n",
    "    process = subprocess.Popen(\n",
    "        [sys.executable, '-u', 'main.py'],\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT, # Merge stderr into stdout\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # Send Model Name input\n",
    "    try:\n",
    "        process.stdin.write('MSBEGCL\\n')\n",
    "        process.stdin.flush()\n",
    "        process.stdin.close()\n",
    "    except Exception: pass\n",
    "    \n",
    "    # 3. Monitor Output\n",
    "    run_recall = 0.0\n",
    "    start_time = time.time()\n",
    "    line_count = 0\n",
    "    \n",
    "    while True:\n",
    "        line = process.stdout.readline()\n",
    "        if not line and process.poll() is not None: break\n",
    "        \n",
    "        if line:\n",
    "            l = line.strip()\n",
    "            line_count += 1\n",
    "            # ALWAYS PRINT FIRST 50 LINES for debugging\n",
    "            if line_count < 50:\n",
    "                 print(f\"   [Debug]: {l}\")\n",
    "            \n",
    "            # Log interesting lines\n",
    "            if \"Recall\" in l or \"Traceback\" in l or \"Error\" in l or \"Epoch\" in l:\n",
    "                if line_count >= 50: # Avoid dupes\n",
    "                     print(f\"   [Log]: {l[:100]}...\") # truncate for cleanliness\n",
    "                \n",
    "            # Parse Recall\n",
    "            # Format usually: \"Performance: Recall@[10, 20]: [0.xxx, 0.xxx]\" or similar\n",
    "            if \"Recall\" in l:\n",
    "                try:\n",
    "                    # Find all floats\n",
    "                    nums = [float(x) for x in re.findall(r\"0\\.\\d+\", l)]\n",
    "                    if nums:\n",
    "                        # Assuming the metrics are rising or the last ones are @20\n",
    "                        # Usually Recall@10, Recall@20. So max is likely @20\n",
    "                        current_max = max(nums)\n",
    "                        run_recall = max(run_recall, current_max)\n",
    "                except: pass\n",
    "                \n",
    "        # Timeout safety (2 minutes per run is enough for 1 epoch)\n",
    "        if time.time() - start_time > 180:\n",
    "            process.kill()\n",
    "            print(\"   [Timeout] Killed process.\")\n",
    "            break\n",
    "            \n",
    "    # Check Result\n",
    "    if run_recall > 0.065:\n",
    "        print(f\"   [!!!] SUCCESS! Params ({lam}, {gam}, {tau}) -> Recall {run_recall}\")\n",
    "        results_log.append({\n",
    "            'lambda': lam,\n",
    "            'gamma': gam,\n",
    "            'tau': tau,\n",
    "            'recall': run_recall\n",
    "        })\n",
    "        found_target = True\n",
    "    else:\n",
    "        print(f\"   [x] Failed. Best Recall: {run_recall}\")\n",
    "\n",
    "print(\"\\n\\n========================================\")\n",
    "print(\"       AUTO-TUNER FINAL REPORT          \")\n",
    "print(\"========================================\")\n",
    "\n",
    "if not results_log:\n",
    "    print(\"No parameters achieved > 0.065 recall in early epochs.\")\n",
    "else:\n",
    "    print(f\"Found {len(results_log)} successful configurations:\\n\")\n",
    "    for res in results_log:\n",
    "        print(f\"Params: lambda={res['lambda']}, gamma={res['gamma']}, tau={res['tau']}\")\n",
    "        print(f\"Result: Recall@20 = {res['recall']}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
