{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d690e21",
   "metadata": {},
   "source": [
    "# Run MSBEGCL on Kaggle\n",
    "\n",
    "This notebook sets up the environment, compiles the necessary C++ mining tools, prepares the data, runs the mining algorithm to generate bicliques, and finally trains the MSBEGCL recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, time, shutil, struct\n",
    "\n",
    "# --- Configuration ---\n",
    "repo_url = 'https://github.com/yangzeha/MSBEGCL.git'\n",
    "repo_dir = 'MSBEGCL'\n",
    "model_name = 'MSBEGCL'\n",
    "dataset_name = 'yelp2018'\n",
    "\n",
    "# 1. Clean and Clone Repository\n",
    "if os.path.exists(repo_dir):\n",
    "    print(f\"Removing existing '{repo_dir}' to ensure a fresh clone...\")\n",
    "    try:\n",
    "        shutil.rmtree(repo_dir)\n",
    "        print(\"Removal successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing directory: {e}\")\n",
    "        subprocess.run(['rm', '-rf', repo_dir])\n",
    "\n",
    "print(f'Cloning {repo_dir} from {repo_url} (branch: master)...')\n",
    "try:\n",
    "    subprocess.run(['git', 'clone', '-b', 'master', repo_url], check=True)\n",
    "    print(\"Clone successful.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Git clone failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 2. Setup Directories\n",
    "if os.path.basename(os.getcwd()) != repo_dir:\n",
    "    os.chdir(repo_dir)\n",
    "print(f'Initial working directory: {os.getcwd()}')\n",
    "\n",
    "# [Robustness Fix]: Auto-detect nested structure\n",
    "roots = os.listdir('.')\n",
    "target_structure_found = False\n",
    "possible_subdirs = ['.', 'MSBEGCL', 'msbegcl', repo_dir]\n",
    "\n",
    "for d in possible_subdirs:\n",
    "    if d == '.': path_to_check = '.'\n",
    "    else:\n",
    "        path_to_check = d\n",
    "        if not os.path.exists(d) or not os.path.isdir(d): continue\n",
    "    contents = os.listdir(path_to_check)\n",
    "    if 'SELFRec' in contents and 'Similar-Biclique-Idx' in contents:\n",
    "        print(f\"Project root found in: '{path_to_check}'\")\n",
    "        if d != '.': os.chdir(d)\n",
    "        target_structure_found = True\n",
    "        break\n",
    "\n",
    "if not target_structure_found:\n",
    "    print(\"Searching recursively for SELFRec...\")\n",
    "    found = False\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        if 'SELFRec' in dirs:\n",
    "            print(f\"Found SELFRec in {root}\")\n",
    "            os.chdir(root)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(\"CRITICAL ERROR: Could not locate SELFRec directory anywhere.\")\n",
    "\n",
    "selfrec_path = 'SELFRec'\n",
    "msbe_path = 'Similar-Biclique-Idx'\n",
    "\n",
    "# 3. Install Dependencies\n",
    "print('\\n--- Installing Python Dependencies ---')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'PyYAML==6.0.2', 'scipy==1.14.1', '-q'], check=True)\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'faiss-cpu', '-q'], check=True)\n",
    "except:\n",
    "    print(\"faiss-cpu install failed, continuing...\")\n",
    "\n",
    "# 4. Compile C++ Mining Tools\n",
    "print('\\n--- Compiling C++ Mining Tools ---')\n",
    "sparsez_dir = 'sparsehash'\n",
    "if not os.path.exists(sparsez_dir):\n",
    "    print(\"Cloning sparsehash...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/sparsehash/sparsehash.git'], check=True)\n",
    "    cwd_backup = os.getcwd()\n",
    "    os.chdir(sparsez_dir)\n",
    "    try:\n",
    "        subprocess.run(['chmod', '+x', 'configure']) \n",
    "        subprocess.run(['./configure'], check=True)\n",
    "        subprocess.run(['make'], check=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: sparsehash configure/make failed: {e}. Trying to proceed with raw source...\")\n",
    "    finally:\n",
    "        os.chdir(cwd_backup)\n",
    "\n",
    "# Compile msbe\n",
    "msbe_src = os.path.join(msbe_path, 'main.cpp')\n",
    "msbe_exe = './msbe'\n",
    "if not os.path.exists(msbe_src):\n",
    "    print(f\"CRITICAL ERROR: Source file {msbe_src} not found!\")\n",
    "else:\n",
    "    # Ensure -D_CheckResults_ is present!\n",
    "    subprocess.run(['g++', '-O3', msbe_src, '-o', msbe_exe, '-I', msbe_path, '-I', 'sparsehash/src', '-D_PrintResults_', '-D_CheckResults_'], check=True)\n",
    "    subprocess.run(['chmod', '+x', msbe_exe])\n",
    "    print('msbe compiled.')\n",
    "\n",
    "# 5. Data Preprocessing\n",
    "print(f'\\n--- Preprocessing {dataset_name} for Mining ---')\n",
    "train_file = os.path.join(selfrec_path, 'dataset', dataset_name, 'train.txt')\n",
    "mining_graph_txt = 'graph.txt'\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print(f\"CRITICAL ERROR: Data file {train_file} not found!\")\n",
    "else:\n",
    "    users = set()\n",
    "    items = set()\n",
    "    edges = []\n",
    "    with open(train_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                u, i = parts[0], parts[1]\n",
    "                users.add(u)\n",
    "                items.add(i)\n",
    "                edges.append((u, i))\n",
    "\n",
    "    # [Robustness Fix]: Try numeric sort if possible to align with integer IDs\n",
    "    try:\n",
    "        sorted_users = sorted(list(users), key=lambda x: int(x))\n",
    "        sorted_items = sorted(list(items), key=lambda x: int(x))\n",
    "    except:\n",
    "        sorted_users = sorted(list(users))\n",
    "        sorted_items = sorted(list(items))\n",
    "\n",
    "    u_map = {u: idx for idx, u in enumerate(sorted_users)}\n",
    "    i_map = {i: idx for idx, i in enumerate(sorted_items)}\n",
    "\n",
    "    n1 = len(users)\n",
    "    n2 = len(items)\n",
    "   \n",
    "    # Python-based Binary Generation (Imitating ai_project logic)\n",
    "    print(f'Preprocessing graph with {n1} users, {n2} items, {len(edges)} edges.')\n",
    "    \n",
    "    # 1. Build Adjacency List (Undirected/Bipartite with Global IDs)\n",
    "    # n1 users [0, n1-1], n2 items [n1, n1+n2-1]\n",
    "    total_nodes = n1 + n2\n",
    "    adj = [[] for _ in range(total_nodes)]\n",
    "    edge_count = 0\n",
    "    \n",
    "    for u, i in edges:\n",
    "        uid = u_map[u]\n",
    "        iid = i_map[i] + n1\n",
    "        \n",
    "        # Add undirected edge\n",
    "        adj[uid].append(iid)\n",
    "        adj[iid].append(uid)\n",
    "        edge_count += 2\n",
    "        \n",
    "    # Sort adjacency lists (MSBE requirement)\n",
    "    for k in range(total_nodes):\n",
    "        adj[k].sort()\n",
    "        \n",
    "    # 2. Write _b_degree.bin\n",
    "    degree_file = 'graph_b_degree.bin'\n",
    "    with open(degree_file, 'wb') as f:\n",
    "        f.write(struct.pack('I', 4)) # sizeof(ui)\n",
    "        f.write(struct.pack('I', n1))\n",
    "        f.write(struct.pack('I', n2))\n",
    "        f.write(struct.pack('I', edge_count))\n",
    "        \n",
    "        degrees = [len(adj[k]) for k in range(total_nodes)]\n",
    "        f.write(struct.pack(f'{total_nodes}I', *degrees))\n",
    "        \n",
    "    # 3. Write _b_adj.bin\n",
    "    adj_file = 'graph_b_adj.bin'\n",
    "    with open(adj_file, 'wb') as f:\n",
    "        flat_adj = []\n",
    "        for k in range(total_nodes):\n",
    "            flat_adj.extend(adj[k])\n",
    "        f.write(struct.pack(f'{edge_count}I', *flat_adj))\n",
    "        \n",
    "    print(f\"Generated binary graph files: {degree_file}, {adj_file}\")\n",
    "    \n",
    "    # Create dummy text file to satisfy MSBE input check\n",
    "    with open(mining_graph_txt, 'w') as f:\n",
    "        f.write(\"dummy\")\n",
    "\n",
    "# 6. Run Mining\n",
    "print('\\n--- Mining Bicliques ---')\n",
    "sim_threshold = 0.15  # epsilon (Revised to 0.15 for better coverage)\n",
    "size_threshold = 2   # tau\n",
    "\n",
    "if os.path.exists(msbe_exe) and os.path.exists(mining_graph_txt):\n",
    "    print('Building Index...')\n",
    "    subprocess.run([msbe_exe, mining_graph_txt, '1', '1', '0.3', 'GRL3'], check=True)\n",
    "\n",
    "    print('Enumerating...')\n",
    "    raw_bicliques_file = 'bicliques_raw.txt'\n",
    "    with open(raw_bicliques_file, 'w') as outfile:\n",
    "        subprocess.run([\n",
    "            msbe_exe, mining_graph_txt, \n",
    "            '0', '1', '0.3', 'GRL3', \n",
    "            '1', 'GRL3', \n",
    "            '0', '0', 'heu', \n",
    "            '4', str(sim_threshold), str(size_threshold), '2'\n",
    "        ], stdout=outfile, check=True)\n",
    "    \n",
    "    if os.path.exists(raw_bicliques_file):\n",
    "        size = os.path.getsize(raw_bicliques_file)\n",
    "        print(f\"Mining output file size: {size} bytes\")\n",
    "        # Print first 200 chars for debug\n",
    "        try:\n",
    "             with open(raw_bicliques_file, 'r') as f:\n",
    "                print(f\"First line snippet: {f.read(200)}\")\n",
    "        except: pass\n",
    "else:\n",
    "    print(\"Skipping mining due to compliation or data failure.\")\n",
    "\n",
    "# 7. Process Bicliques -> Model Format\n",
    "print('\\n--- Formatting Bicliques for Model ---')\n",
    "final_biclique_path = os.path.join(selfrec_path, 'dataset', dataset_name, 'bicliques.txt')\n",
    "count = 0\n",
    "\n",
    "if os.path.exists('bicliques_raw.txt'):\n",
    "    with open('bicliques_raw.txt', 'r') as fr, open(final_biclique_path, 'w') as fw:\n",
    "        current_users = []\n",
    "        current_items = []\n",
    "        \n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Universal Parser: Handle any delimiter\n",
    "            clean_line = line.replace('|', ' ').replace(',', ' ').replace(':', ' ')\n",
    "            tokens = clean_line.split()\n",
    "            \n",
    "            current_users = []\n",
    "            current_items = []\n",
    "            \n",
    "            for t in tokens:\n",
    "                if not t.isdigit(): continue\n",
    "                nid = int(t)\n",
    "                \n",
    "                # Check ID range to classify User vs Item\n",
    "                if nid < n1:\n",
    "                    if nid < len(sorted_users):\n",
    "                        current_users.append(sorted_users[nid])\n",
    "                else:\n",
    "                    # Item IDs are offset by n1\n",
    "                    iid = nid - n1\n",
    "                    if iid >= 0 and iid < len(sorted_items):\n",
    "                        current_items.append(sorted_items[iid])\n",
    "            \n",
    "            if len(current_users) > 0 and len(current_items) > 0:\n",
    "                fw.write(f\"{' '.join(current_users)} | {' '.join(current_items)}\\n\")\n",
    "                count += 1\n",
    "                \n",
    "    print(f\"Processed {count} bicliques into {final_biclique_path}\")\n",
    "    \n",
    "    if count == 0:\n",
    "         print(\"CRITICAL ERROR: No bicliques parsed.\")\n",
    "         sys.exit(1)\n",
    "else:\n",
    "    print(\"Warning: bicliques_raw.txt not found.\")\n",
    "\n",
    "# 8. Update Configuration\n",
    "conf_path = os.path.join(selfrec_path, 'conf', 'MSBEGCL.yaml')\n",
    "\n",
    "if not os.path.exists(conf_path):\n",
    "    print(f\"Error: Config file {conf_path} not found.\")\n",
    "else:\n",
    "    with open(conf_path, 'r') as f:\n",
    "        conf_content = f.read()\n",
    "\n",
    "    new_path = f'./dataset/{dataset_name}/bicliques.txt'\n",
    "    import re\n",
    "    \n",
    "    conf_content = re.sub(r'biclique\\.file:.*', f'biclique.file: {new_path}', conf_content)\n",
    "    # Increase epochs\n",
    "    conf_content = re.sub(r'max\\.epoch:.*', 'max.epoch: 40', conf_content)\n",
    "    # [Tuning Fix]: Force n_layer to 2 for Yelp2018 (prevent oversmoothing)\n",
    "    conf_content = re.sub(r'n_layer:.*', 'n_layer: 2', conf_content)\n",
    "\n",
    "    with open(conf_path, 'w') as f:\n",
    "        f.write(conf_content)\n",
    "    print(\"Updated MSBEGCL.yaml.\")\n",
    "\n",
    "# 9. Run MSBEGCL\n",
    "print('\\n--- Starting Training ---')\n",
    "\n",
    "main_py_path = os.path.join(selfrec_path, 'main.py')\n",
    "if not os.path.exists(main_py_path):\n",
    "    print(f\"CRITICAL: {main_py_path} not found.\")\n",
    "\n",
    "os.chdir(selfrec_path)\n",
    "print(f\"Changed directory to {os.getcwd()} for training.\")\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [sys.executable, '-u', 'main.py'],\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT, \n",
    "    text=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "try:\n",
    "    process.stdin.write(f'{model_name}\\n')\n",
    "    process.stdin.flush()\n",
    "    process.stdin.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to stdin: {e}\")\n",
    "\n",
    "while True:\n",
    "    line = process.stdout.readline()\n",
    "    if not line and process.poll() is not None:\n",
    "        break\n",
    "    if line:\n",
    "        print(line.strip())\n",
    "\n",
    "if process.poll() != 0:\n",
    "    print(\"Training failed.\")\n",
    "else:\n",
    "    print(\"Training finished successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
